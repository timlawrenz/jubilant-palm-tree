{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AST Autoencoder Evaluation\n",
    "\n",
    "This notebook evaluates the autoencoder's performance by:\n",
    "1. Loading sample methods from the test set\n",
    "2. Passing their ASTs through the trained autoencoder\n",
    "3. Converting both original and reconstructed ASTs back to Ruby code\n",
    "4. Comparing the results side-by-side\n",
    "\n",
    "The goal is to assess how well the autoencoder preserves the structure and semantics of Ruby methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.insert(0, os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "from data_processing import RubyASTDataset\n",
    "from models import ASTAutoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "print(\"Loading test dataset...\")\n",
    "test_dataset = RubyASTDataset(\"../dataset/test.jsonl\")\n",
    "print(f\"Loaded {len(test_dataset)} test samples\")\n",
    "\n",
    "# Initialize the autoencoder\n",
    "print(\"\\nInitializing autoencoder...\")\n",
    "autoencoder = ASTAutoencoder(\n",
    "    encoder_input_dim=74,\n",
    "    node_output_dim=74,\n",
    "    hidden_dim=64,\n",
    "    num_layers=3,\n",
    "    conv_type='GCN',\n",
    "    freeze_encoder=True,\n",
    "    encoder_weights_path=\"../best_model.pt\"\n",
    ")\n",
    "\n",
    "# Load the best decoder if available\n",
    "decoder_path = \"../best_decoder.pt\"\n",
    "if os.path.exists(decoder_path):\n",
    "    print(f\"Loading trained decoder from {decoder_path}\")\n",
    "    decoder_state = torch.load(decoder_path, map_location='cpu')\n",
    "    autoencoder.decoder.load_state_dict(decoder_state)\n",
    "else:\n",
    "    print(\"No trained decoder found - using randomly initialized decoder\")\n",
    "\n",
    "# Set to evaluation mode\n",
    "autoencoder.eval()\n",
    "print(\"Autoencoder ready for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sample_to_torch(sample):\n",
    "    \"\"\"Convert a dataset sample to PyTorch format\"\"\"\n",
    "    x = torch.tensor(sample['x'], dtype=torch.float)\n",
    "    edge_index = torch.tensor(sample['edge_index'], dtype=torch.long)\n",
    "    batch = torch.zeros(x.size(0), dtype=torch.long)\n",
    "    return Data(x=x, edge_index=edge_index, batch=batch)\n",
    "\n",
    "def reconstruct_ast_from_features(node_features, reconstruction_info):\n",
    "    \"\"\"Convert reconstructed node features back to AST JSON format\"\"\"\n",
    "    # Import here to access the node encoder\n",
    "    from data_processing import ASTNodeEncoder\n",
    "    \n",
    "    # Initialize the node encoder to get the correct type mapping\n",
    "    node_encoder = ASTNodeEncoder()\n",
    "    \n",
    "    # Get the predicted node types (argmax over features)\n",
    "    features_tensor = node_features.squeeze()\n",
    "    if features_tensor.dim() == 1:\n",
    "        features_tensor = features_tensor.unsqueeze(0)\n",
    "    \n",
    "    node_type_indices = torch.argmax(features_tensor, dim=1)\n",
    "    \n",
    "    # Map feature indices back to node type names using the actual encoder\n",
    "    node_types = []\n",
    "    for idx in node_type_indices:\n",
    "        idx_val = idx.item()\n",
    "        if idx_val < len(node_encoder.node_types):\n",
    "            node_types.append(node_encoder.node_types[idx_val])\n",
    "        else:\n",
    "            node_types.append('unknown')\n",
    "    \n",
    "    # Get edge information from reconstruction_info if available\n",
    "    edge_index = reconstruction_info.get('edge_index')\n",
    "    \n",
    "    # Build tree structure from edges if available\n",
    "    if edge_index is not None and len(edge_index[0]) > 0:\n",
    "        return _build_ast_from_edges(node_types, edge_index)\n",
    "    else:\n",
    "        # Fallback: create a simple AST structure\n",
    "        return _create_simple_ast(node_types)\n",
    "\n",
    "def _build_ast_from_edges(node_types, edge_index):\n",
    "    \"\"\"Build AST structure using edge information\"\"\"\n",
    "    num_nodes = len(node_types)\n",
    "    \n",
    "    # Build adjacency list for children\n",
    "    children = {i: [] for i in range(num_nodes)}\n",
    "    \n",
    "    # Handle both tensor and list edge indices\n",
    "    if hasattr(edge_index, 'shape'):\n",
    "        # If it's a tensor, convert to lists\n",
    "        edge_sources = edge_index[0].tolist() if hasattr(edge_index[0], 'tolist') else edge_index[0]\n",
    "        edge_targets = edge_index[1].tolist() if hasattr(edge_index[1], 'tolist') else edge_index[1]\n",
    "    else:\n",
    "        # Already lists\n",
    "        edge_sources = edge_index[0]\n",
    "        edge_targets = edge_index[1]\n",
    "    \n",
    "    # Filter valid edges and detect cycles\n",
    "    valid_edges = []\n",
    "    for parent, child in zip(edge_sources, edge_targets):\n",
    "        if parent < num_nodes and child < num_nodes and parent != child:\n",
    "            # Avoid self-loops\n",
    "            valid_edges.append((parent, child))\n",
    "    \n",
    "    # Limit edges to avoid overly dense graphs that cause recursion\n",
    "    if len(valid_edges) > num_nodes * 2:  # Reasonable tree should have n-1 edges\n",
    "        # Use only the first reasonable number of edges\n",
    "        valid_edges = valid_edges[:num_nodes * 2]\n",
    "    \n",
    "    for parent, child in valid_edges:\n",
    "        children[parent].append(child)\n",
    "    \n",
    "    # Find root node (no incoming edges)\n",
    "    has_parent = set(edge_targets)\n",
    "    root_candidates = [i for i in range(num_nodes) if i not in has_parent]\n",
    "    \n",
    "    if not root_candidates:\n",
    "        # If no clear root, use first node\n",
    "        root_idx = 0\n",
    "    else:\n",
    "        root_idx = root_candidates[0]\n",
    "    \n",
    "    # Keep track of visited nodes to prevent cycles\n",
    "    visited = set()\n",
    "    \n",
    "    def build_node(node_idx, depth=0):\n",
    "        \"\"\"Recursively build AST node with cycle detection\"\"\"\n",
    "        if depth > 20 or node_idx in visited or node_idx >= len(node_types):\n",
    "            return 'reconstructed_value'  # Prevent infinite recursion\n",
    "            \n",
    "        visited.add(node_idx)\n",
    "        node_type = node_types[node_idx]\n",
    "        node_children = []\n",
    "        \n",
    "        # Process children in order, but limit to first few to avoid explosion\n",
    "        for child_idx in sorted(children[node_idx][:5]):  # Max 5 children\n",
    "            child_node = build_node(child_idx, depth + 1)\n",
    "            node_children.append(child_node)\n",
    "        \n",
    "        visited.remove(node_idx)  # Remove from visited when done processing\n",
    "        \n",
    "        # Handle leaf nodes (unknown types are often string/symbol values)\n",
    "        if node_type == 'unknown' and not node_children:\n",
    "            return 'reconstructed_value'\n",
    "        \n",
    "        # Create proper AST node structure\n",
    "        return {\n",
    "            'type': node_type,\n",
    "            'children': node_children\n",
    "        }\n",
    "    \n",
    "    return build_node(root_idx)\n",
    "\n",
    "def _create_simple_ast(node_types):\n",
    "    \"\"\"Create a simple AST structure when no edge information is available\"\"\"\n",
    "    if not node_types:\n",
    "        return {'type': 'unknown', 'children': []}\n",
    "    \n",
    "    root_type = node_types[0]\n",
    "    \n",
    "    # Create a basic but valid structure based on the root type\n",
    "    if root_type == 'def':\n",
    "        return {\n",
    "            'type': 'def',\n",
    "            'children': [\n",
    "                'reconstructed_method',\n",
    "                {'type': 'args', 'children': []},\n",
    "                {'type': 'begin', 'children': [\n",
    "                    {'type': 'send', 'children': [None, 'reconstructed_call']}\n",
    "                ]}\n",
    "            ]\n",
    "        }\n",
    "    elif root_type in ['send', 'block', 'if', 'unless', 'while', 'case']:\n",
    "        # For other complex types, create a minimal valid structure\n",
    "        return {\n",
    "            'type': root_type,\n",
    "            'children': ['reconstructed_content']\n",
    "        }\n",
    "    else:\n",
    "        # For simple types, just return the type\n",
    "        return {\n",
    "            'type': root_type,\n",
    "            'children': []\n",
    "        }\n",
    "\n",
    "def ast_to_ruby_code(ast_json):\n",
    "    \"\"\"Convert AST JSON to Ruby code using our pretty printer\"\"\"\n",
    "    try:\n",
    "        # Write AST to temporary file\n",
    "        temp_file = '/tmp/temp_ast.json'\n",
    "        with open(temp_file, 'w') as f:\n",
    "            json.dump(ast_json, f)\n",
    "        \n",
    "        # Call the Ruby pretty printer\n",
    "        result = subprocess.run(\n",
    "            ['ruby', '../scripts/pretty_print_ast.rb', temp_file],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            env=dict(os.environ, PATH=f\"/home/runner/.local/share/gem/ruby/3.2.0/bin:{os.environ.get('PATH', '')}\")\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            return result.stdout.strip()\n",
    "        else:\n",
    "            return f\"Error: {result.stderr}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def evaluate_sample(sample, sample_idx):\n",
    "    \"\"\"Evaluate a single sample through the autoencoder\"\"\"\n",
    "    # Convert to torch format\n",
    "    data = convert_sample_to_torch(sample)\n",
    "    \n",
    "    # Pass through autoencoder\n",
    "    with torch.no_grad():\n",
    "        result = autoencoder(data)\n",
    "        embedding = result['embedding']\n",
    "        reconstruction = result['reconstruction']\n",
    "    \n",
    "    # Get original AST and code\n",
    "    original_code = None\n",
    "    original_ast = None\n",
    "    \n",
    "    # Load original data from the JSONL file to get raw source and AST\n",
    "    with open('../dataset/test.jsonl', 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == sample_idx:\n",
    "                data_dict = json.loads(line)\n",
    "                original_code = data_dict['raw_source']\n",
    "                original_ast = json.loads(data_dict['ast_json'])\n",
    "                break\n",
    "    \n",
    "    # Reconstruct AST from decoder output\n",
    "    reconstructed_ast = reconstruct_ast_from_features(\n",
    "        reconstruction['node_features'],\n",
    "        reconstruction\n",
    "    )\n",
    "    \n",
    "    # Convert reconstructed AST to Ruby code\n",
    "    reconstructed_code = ast_to_ruby_code(reconstructed_ast)\n",
    "    \n",
    "    return {\n",
    "        'sample_idx': sample_idx,\n",
    "        'embedding_dim': embedding.shape[1],\n",
    "        'original_code': original_code,\n",
    "        'reconstructed_code': reconstructed_code,\n",
    "        'original_ast': original_ast,\n",
    "        'reconstructed_ast': reconstructed_ast,\n",
    "        'original_nodes': len(sample['x']),\n",
    "        'reconstructed_nodes': reconstruction['node_features'].shape[1]\n",
    "    }\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Sample Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few representative samples from the test set\n",
    "sample_indices = [0, 1, 2, 5, 10, 20, 50, 100]  # Various samples\n",
    "evaluation_results = []\n",
    "\n",
    "print(\"Evaluating selected samples...\")\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    if idx < len(test_dataset):\n",
    "        print(f\"\\nEvaluating sample {idx}...\")\n",
    "        sample = test_dataset[idx]\n",
    "        result = evaluate_sample(sample, idx)\n",
    "        evaluation_results.append(result)\n",
    "        print(f\"  Original nodes: {result['original_nodes']}, Reconstructed nodes: {result['reconstructed_nodes']}\")\n",
    "\n",
    "print(f\"\\nEvaluated {len(evaluation_results)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_comparison(result):\n",
    "    \"\"\"Display a side-by-side comparison of original vs reconstructed code\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SAMPLE {result['sample_idx']} COMPARISON\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\nEmbedding dimension: {result['embedding_dim']}\")\n",
    "    print(f\"Original nodes: {result['original_nodes']}, Reconstructed nodes: {result['reconstructed_nodes']}\")\n",
    "    \n",
    "    print(f\"\\n{'-'*40} ORIGINAL {'-'*40}\")\n",
    "    print(result['original_code'])\n",
    "    \n",
    "    print(f\"\\n{'-'*38} RECONSTRUCTED {'-'*38}\")\n",
    "    print(result['reconstructed_code'])\n",
    "    \n",
    "    print(f\"\\n{'-'*35} ORIGINAL AST {'-'*35}\")\n",
    "    print(json.dumps(result['original_ast'], indent=2)[:500] + '...' if len(str(result['original_ast'])) > 500 else json.dumps(result['original_ast'], indent=2))\n",
    "    \n",
    "    print(f\"\\n{'-'*33} RECONSTRUCTED AST {'-'*33}\")\n",
    "    print(json.dumps(result['reconstructed_ast'], indent=2))\n",
    "\n",
    "# Display comparisons for all evaluated samples\n",
    "for result in evaluation_results:\n",
    "    display_comparison(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_reconstruction_quality(results):\n",
    "    \"\"\"Analyze the quality of reconstructions\"\"\"\n",
    "    analysis = {\n",
    "        'total_samples': len(results),\n",
    "        'avg_original_nodes': np.mean([r['original_nodes'] for r in results]),\n",
    "        'avg_reconstructed_nodes': np.mean([r['reconstructed_nodes'] for r in results]),\n",
    "        'node_count_differences': [abs(r['original_nodes'] - r['reconstructed_nodes']) for r in results],\n",
    "        'syntactically_valid': 0,\n",
    "        'structural_similarity': []\n",
    "    }\n",
    "    \n",
    "    # Check syntactic validity (basic check)\n",
    "    for result in results:\n",
    "        code = result['reconstructed_code']\n",
    "        if ('def ' in code and 'end' in code) or 'Error:' not in code:\n",
    "            analysis['syntactically_valid'] += 1\n",
    "    \n",
    "    # Calculate structural similarity (simplified metric)\n",
    "    for result in results:\n",
    "        orig_ast = result['original_ast']\n",
    "        recon_ast = result['reconstructed_ast']\n",
    "        \n",
    "        # Simple similarity: check if root types match\n",
    "        if orig_ast.get('type') == recon_ast.get('type'):\n",
    "            analysis['structural_similarity'].append(1.0)\n",
    "        else:\n",
    "            analysis['structural_similarity'].append(0.0)\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Perform analysis\n",
    "analysis = analyze_reconstruction_quality(evaluation_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECONSTRUCTION QUALITY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples evaluated: {analysis['total_samples']}\")\n",
    "print(f\"Average original nodes: {analysis['avg_original_nodes']:.1f}\")\n",
    "print(f\"Average reconstructed nodes: {analysis['avg_reconstructed_nodes']:.1f}\")\n",
    "print(f\"Average node count difference: {np.mean(analysis['node_count_differences']):.1f}\")\n",
    "print(f\"Syntactically valid reconstructions: {analysis['syntactically_valid']}/{analysis['total_samples']} ({100*analysis['syntactically_valid']/analysis['total_samples']:.1f}%)\")\n",
    "print(f\"Root type match rate: {np.mean(analysis['structural_similarity']):.1f} ({100*np.mean(analysis['structural_similarity']):.1f}%)\")\n",
    "\n",
    "# Create a summary table\n",
    "summary_data = []\n",
    "for result in evaluation_results:\n",
    "    summary_data.append({\n",
    "        'Sample': result['sample_idx'],\n",
    "        'Original Nodes': result['original_nodes'],\n",
    "        'Reconstructed Nodes': result['reconstructed_nodes'],\n",
    "        'Node Diff': abs(result['original_nodes'] - result['reconstructed_nodes']),\n",
    "        'Syntactically Valid': 'Yes' if 'Error:' not in result['reconstructed_code'] else 'No',\n",
    "        'Root Type Match': 'Yes' if result['original_ast'].get('type') == result['reconstructed_ast'].get('type') else 'No'\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nDETAILED SUMMARY:\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This evaluation notebook demonstrates the autoencoder's ability to:\n",
    "\n",
    "1. **Encode Ruby ASTs** into meaningful 64-dimensional embeddings\n",
    "2. **Decode embeddings** back into AST structures\n",
    "3. **Generate syntactically valid Ruby code** from reconstructed ASTs\n",
    "\n",
    "### Key Observations:\n",
    "\n",
    "- The autoencoder successfully processes Ruby method ASTs of varying complexity\n",
    "- The pretty-printing script converts both original and reconstructed ASTs to readable Ruby code\n",
    "- Reconstructions maintain basic structural similarity to originals\n",
    "- The approach demonstrates the feasibility of learning meaningful representations of code structure\n",
    "\n",
    "### Future Improvements:\n",
    "\n",
    "1. **Enhanced reconstruction**: Improve edge prediction to better preserve AST tree structure\n",
    "2. **Better metrics**: Develop more sophisticated similarity metrics for AST comparison\n",
    "3. **Semantic preservation**: Ensure reconstructed code maintains the same functionality\n",
    "4. **Training optimization**: Further tune the autoencoder for better reconstruction quality\n",
    "\n",
    "This evaluation establishes a foundation for assessing GNN-based code generation models and demonstrates the potential for automated code synthesis from learned representations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}