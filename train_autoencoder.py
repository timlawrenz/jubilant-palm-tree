#!/usr/bin/env python3
"""
Training script for AST Autoencoder using Graph Neural Networks.

This script implements the training loop for the ASTAutoencoder model that
reconstructs Ruby method ASTs from learned embeddings. It uses a frozen encoder
and only trains the decoder weights.
"""

import sys
import os
import time
import argparse
import torch
import torch.nn.functional as F
from torch_geometric.data import Data

# Add src directory to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from data_processing import create_data_loaders
from models import ASTAutoencoder
from loss import ast_reconstruction_loss_simple


def train_epoch(model, train_loader, optimizer, device):
    """
    Train the autoencoder for one epoch.
    
    Args:
        model: The ASTAutoencoder model
        train_loader: Training data loader
        optimizer: Optimizer instance
        device: Device to run on
        
    Returns:
        Average training loss for the epoch
    """
    model.train()
    total_loss = 0.0
    num_batches = 0
    
    for batch in train_loader:
        # Convert to PyTorch tensors and move to device
        x = torch.tensor(batch['x'], dtype=torch.float).to(device)
        edge_index = torch.tensor(batch['edge_index'], dtype=torch.long).to(device)
        batch_idx = torch.tensor(batch['batch'], dtype=torch.long).to(device)
        
        # Create PyTorch Geometric Data object
        data = Data(x=x, edge_index=edge_index, batch=batch_idx)
        
        # Forward pass through autoencoder
        optimizer.zero_grad()
        result = model(data)
        
        # Compute reconstruction loss (input and target are the same AST)
        loss = ast_reconstruction_loss_simple(data, result['reconstruction'])
        
        # Backward pass (only decoder weights will be updated)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        num_batches += 1
    
    return total_loss / num_batches if num_batches > 0 else 0.0


def validate_epoch(model, val_loader, device):
    """
    Validate the autoencoder for one epoch.
    
    Args:
        model: The ASTAutoencoder model
        val_loader: Validation data loader
        device: Device to run on
        
    Returns:
        Average validation loss for the epoch
    """
    model.eval()
    total_loss = 0.0
    num_batches = 0
    
    with torch.no_grad():
        for batch in val_loader:
            # Convert to PyTorch tensors and move to device
            x = torch.tensor(batch['x'], dtype=torch.float).to(device)
            edge_index = torch.tensor(batch['edge_index'], dtype=torch.long).to(device)
            batch_idx = torch.tensor(batch['batch'], dtype=torch.long).to(device)
            
            # Create PyTorch Geometric Data object
            data = Data(x=x, edge_index=edge_index, batch=batch_idx)
            
            # Forward pass
            result = model(data)
            loss = ast_reconstruction_loss_simple(data, result['reconstruction'])
            
            total_loss += loss.item()
            num_batches += 1
    
    return total_loss / num_batches if num_batches > 0 else 0.0


def save_decoder_weights(model, filepath, epoch, train_loss, val_loss):
    """
    Save decoder weights and training metadata.
    
    Args:
        model: The autoencoder model
        filepath: Path to save the decoder weights
        epoch: Current epoch number
        train_loss: Training loss
        val_loss: Validation loss
    """
    torch.save({
        'epoch': epoch,
        'decoder_state_dict': model.decoder.state_dict(),
        'train_loss': train_loss,
        'val_loss': val_loss,
        'model_config': {
            'embedding_dim': model.decoder.embedding_dim,
            'output_node_dim': model.decoder.output_node_dim,
            'hidden_dim': model.decoder.hidden_dim,
            'num_layers': model.decoder.num_layers,
            'max_nodes': model.decoder.max_nodes
        }
    }, filepath)


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description='Train AST Autoencoder model')
    parser.add_argument('--dataset_path', type=str, default='dataset/',
                        help='Path to dataset directory (default: dataset/)')
    parser.add_argument('--epochs', type=int, default=100,
                        help='Number of training epochs (default: 100)')
    parser.add_argument('--output_path', type=str, default='models/best_decoder.pt',
                        help='Path to save the best decoder model (default: models/best_decoder.pt)')
    parser.add_argument('--encoder_weights_path', type=str, default='models/best_model.pt',
                        help='Path to pre-trained encoder weights (default: models/best_model.pt)')
    parser.add_argument('--batch_size', type=int, default=32,
                        help='Batch size for training (default: 32)')
    parser.add_argument('--learning_rate', type=float, default=0.001,
                        help='Learning rate (default: 0.001)')
    parser.add_argument('--hidden_dim', type=int, default=64,
                        help='Hidden dimension size (default: 64)')
    parser.add_argument('--num_layers', type=int, default=3,
                        help='Number of GNN layers (default: 3)')
    parser.add_argument('--conv_type', type=str, default='GCN', choices=['GCN', 'SAGE'],
                        help='GNN convolution type (default: GCN)')
    parser.add_argument('--dropout', type=float, default=0.1,
                        help='Dropout rate (default: 0.1)')
    return parser.parse_args()


def main():
    """Main training function."""
    args = parse_args()
    
    print("üöÄ AST Autoencoder Training")
    print("=" * 50)
    
    # Training configuration from args
    config = {
        'epochs': args.epochs,
        'batch_size': args.batch_size,
        'learning_rate': args.learning_rate,
        'hidden_dim': args.hidden_dim,
        'num_layers': args.num_layers,
        'conv_type': args.conv_type,
        'dropout': args.dropout,
        'freeze_encoder': True,  # Key requirement: freeze encoder
        'encoder_weights_path': args.encoder_weights_path
    }
    
    print("üìã Training Configuration:")
    for key, value in config.items():
        print(f"   {key}: {value}")
    print(f"   dataset_path: {args.dataset_path}")
    print(f"   output_path: {args.output_path}")
    print()
    
    # Setup device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üñ•Ô∏è  Using device: {device}")
    
    # Create data loaders
    print("üìÇ Loading datasets...")
    train_data_path = os.path.join(args.dataset_path, "train.jsonl")
    val_data_path = os.path.join(args.dataset_path, "validation.jsonl")
    
    train_loader, val_loader = create_data_loaders(
        train_data_path,
        val_data_path,
        batch_size=config['batch_size'],
        shuffle=True
    )
    
    print(f"   Training batches: {len(train_loader)}")
    print(f"   Validation batches: {len(val_loader)}")
    print()
    
    # Initialize autoencoder model
    print("üß† Initializing AST Autoencoder...")
    model = ASTAutoencoder(
        encoder_input_dim=74,  # AST node feature dimension
        node_output_dim=74,    # Reconstruct same dimension
        hidden_dim=config['hidden_dim'],
        num_layers=config['num_layers'],
        conv_type=config['conv_type'],
        dropout=config['dropout'],
        freeze_encoder=config['freeze_encoder'],
        encoder_weights_path=config['encoder_weights_path']
    ).to(device)
    
    # Count parameters
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    frozen_params = total_params - trainable_params
    
    print(f"   Model: {model.get_model_info()}")
    print(f"   Total parameters: {total_params:,}")
    print(f"   Trainable parameters: {trainable_params:,} (decoder only)")
    print(f"   Frozen parameters: {frozen_params:,} (encoder)")
    print()
    
    # Setup optimizer (only for decoder parameters)
    optimizer = torch.optim.Adam(
        filter(lambda p: p.requires_grad, model.parameters()), 
        lr=config['learning_rate']
    )
    
    print("‚öôÔ∏è  Training setup:")
    print(f"   Optimizer: Adam (lr={config['learning_rate']}) - decoder only")
    print(f"   Loss function: AST Reconstruction Loss (simple)")
    print(f"   Input/Target: Same AST graph (autoencoder)")
    print()
    
    # Ensure output directory exists
    os.makedirs(os.path.dirname(args.output_path), exist_ok=True)
    
    # Training loop
    print("üèãÔ∏è  Starting training...")
    print("=" * 50)
    
    best_val_loss = float('inf')
    start_time = time.time()
    
    for epoch in range(config['epochs']):
        epoch_start = time.time()
        
        # Train for one epoch
        train_loss = train_epoch(model, train_loader, optimizer, device)
        
        # Validate
        val_loss = validate_epoch(model, val_loader, device)
        
        epoch_time = time.time() - epoch_start
        
        # Print results for each epoch (required by Definition of Done)
        print(f"Epoch {epoch+1:2d}/{config['epochs']} | "
              f"Train Loss: {train_loss:.4f} | "
              f"Val Loss: {val_loss:.4f} | "
              f"Time: {epoch_time:.2f}s")
        
        # Save best decoder weights (required by Definition of Done)
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            save_decoder_weights(model, args.output_path, epoch, train_loss, val_loss)
            print(f"   üíæ New best decoder saved (val_loss: {val_loss:.4f})")
    
    total_time = time.time() - start_time
    
    print("=" * 50)
    print("üéâ Training completed successfully!")
    print(f"   Total time: {total_time:.2f}s")
    print(f"   Best validation loss: {best_val_loss:.4f}")
    print(f"   Best decoder weights saved to: {args.output_path}")
    
    # Final decoder save (optional, keeping for compatibility)
    final_path = args.output_path.replace('.pt', '_final.pt')
    save_decoder_weights(model, final_path, config['epochs']-1, train_loss, val_loss)
    print(f"   Final decoder weights saved to: {final_path}")
    
    # Verify training objectives
    print("\n‚úÖ Training Objectives Met:")
    print(f"   ‚úì Trained for {config['epochs']} epochs (‚â•2 required)")
    print(f"   ‚úì Only decoder weights trained (encoder frozen)")
    print(f"   ‚úì Used AST reconstruction loss function")
    print(f"   ‚úì Input and target are same AST graph")
    print(f"   ‚úì Best decoder weights saved to {args.output_path}")
    if config['epochs'] > 1:
        print(f"   ‚úì Training completed successfully over multiple epochs")


if __name__ == "__main__":
    main()
